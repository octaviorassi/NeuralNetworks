{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e58523-b5b8-4fc2-a9af-aac843691d70",
   "metadata": {},
   "source": [
    "# Trabajo Práctico de Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5c85c-7594-4f4e-8679-22c3a4b30b48",
   "metadata": {},
   "source": [
    "Las librerías que utilizaremos serán las siguientes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d24d826-a807-4c48-80a2-0af072824c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c1772-2ff4-4885-ae87-babe8089d43d",
   "metadata": {},
   "source": [
    "Y las red neuronales por defecto serán definida por los siguientes parámetros y funciones,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954afda6-c9b9-4e7d-9f4b-1f2fee6b1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros estándar\n",
    "epocas_por_entrenamiento = 25    # numero de epocas que entrena cada vez\n",
    "eta  = 0.01                      # learning rate\n",
    "alfa = 0.9                       # momentum\n",
    "N2   = 60                        # neuronas en la capa oculta\n",
    "\n",
    "def standardRegressionNetwork():\n",
    "    return MLPRegressor(hidden_layer_sizes=(N2,), activation='logistic', solver='sgd', alpha=0.0, batch_size=1,\n",
    "                        learning_rate='constant', learning_rate_init=eta,momentum=alfa,nesterovs_momentum=False,\n",
    "                        tol=0.0,warm_start=True,max_iter=epocas_por_entrenamiento)\n",
    "\n",
    "def standardClassifierNetwork():\n",
    "    return MLPClassifier(hidden_layer_sizes=(N2,), activation='logistic', solver='sgd', alpha=0.0, batch_size=1,\n",
    "                         learning_rate='constant', learning_rate_init=eta,momentum=alfa,nesterovs_momentum=False,\n",
    "                         tol=0.0,warm_start=True,max_iter=epocas_por_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e11671-5dff-4846-bf99-ecbcf543c441",
   "metadata": {},
   "source": [
    "# Problema 1 - Capacidad de modelado\n",
    "\n",
    "Entrene redes neuronales para resolver el problema de clasificación de las espirales anidadas que creamos en el TP 0. Use un número creciente de neuronas en la capa intermedia: 2, 10, 20, 40. Valores posibles para los demás parámetros de entrenamiento: learning rate 0.1, momentum 0.9, 600 datos para ajustar los modelos (20% de ese conjunto separarlo al azar para conjunto de validación), 2000 para testear, 1000 evaluaciones del entrenamiento, cada una de 20 épocas. Para cada uno de los cuatro modelos obtenidos, graficar en el plano xy las clasificaciones sobre el conjunto de test. Comentar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25aabf-5cad-40ae-b210-c98f18cf1d2e",
   "metadata": {},
   "source": [
    "# Problema 2 - Mínimos locales\n",
    "\n",
    "Baje el dataset dos-elipses de la descargas. Realice varios entrenamientos con los siguientes parámetros: 6 neuronas en la capa intermedia, 500 patrones en el training set, de los cuales 400 se usan para entrenar y 100 para validar el modelo (sacados del .data), 2000 patrones en el test set (del .test), 300 evaluaciones del entrenamiento, cada una de 50 épocas. \n",
    "\n",
    "Pruebe distintos valores de momentum y learning-rate (valores usuales son 0, 0.5, 0.9 para el momentum y 0.1, 0.01, 0.001 para el learning-rate, pero no hay por qué limitarse a esos valores), para tratar de encontrar el mejor mínimo posible de la función error. El valor que vamos a usar es el promedio de 10 entrenamientos iguales, dado que los entrenamientos incorporan el azar. Como guía, con los parámetros dados, hay soluciones entre 5% y 6% de error en test, y tal vez mejores. Confeccione una tabla con los valores usados para los parámetros y el resultado en test obtenido (la media de las 10 ejecuciones). Haga una gráfica de error de train, validación y test en función del número de épocas para los valores seleccionados (los mejores valores de eta y alfa). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ef035-0586-4748-bccf-edabce1637a6",
   "metadata": {},
   "source": [
    "# Problema 3 - Regularización\n",
    "\n",
    "Baje el dataset Ikeda (en todos los problemas de regresión la última columna del dataset es la variable a precedir). Realice varios entrenamientos usando el 95% del archivo .data para entrenar, y el resto para validar. Realice otros entrenamientos cambiando la relación a 75%-25%, y a 50%-50%. En cada caso seleccione un resultado que considere adecuado, y genere gráficas del mse en train, validación y test. Comente sobre los resultados. \n",
    "Los otros parámetros para el entrenamiento son: learning rate 0.01, momentum 0.9, 2000 datos para testear, 400 evaluaciones del entrenamiento, cada una de 50 épocas, 30 neuronas en la capa oculta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78bde0-a5dd-4ab1-b35e-3d65bd7b9bbc",
   "metadata": {},
   "source": [
    "# Problema 4 - Regularización \n",
    "Vamos a usar regularización por penalización, el weight-decay. Hay que tener cuidado con los nombres de los parámetros en este caso. El parámetro que nosotros llamamos gamma en la teoría corresponde en MLP de sklearn al parámetro alpha, mientras que nosotros usamos alfa para el momentum en general. Para activarlo tenemos que usar,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc66787-3ab0-46a4-a2b9-b543b756dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.00001\n",
    "regr = MLPRegressor(hidden_layer_sizes=(N2,), activation='logistic', solver='sgd', alpha=gamma, batch_size=1, learning_rate='constant', learning_rate_init=eta,momentum=alfa,nesterovs_momentum=False,tol=0.0,warm_start=True,max_iter=epocas_por_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f5c98-c0c1-44cf-a436-549396d34902",
   "metadata": {},
   "source": [
    "En este tipo de regularización no se usa un conjunto de validación, asi que hay que modificar la función que crearon para evaluar el entrenamiento de las redes, para que en lugar del error sobre el conjunto de validación, nos devuelva la suma de los valores absolutos o de los valores al cuadrado de todos los pesos de la red en la epoca correspondiente, y todo el resto igual que antes.\n",
    "\n",
    "Una vez implementado, aplíquelo al dataset Sunspots (ssp). Busque el valor de gamma adecuado para conseguir un equilibrio entre evitar el sobreajuste y hacer el modelo demasiado rígido (el valor de gamma se debe variar en varios órdenes de magnitud, por ejemplo empezar en 10^-6 e ir hasta 10^0 (1) de a un orden cada vez). En este caso todos los registros del archivo .data se deben usar para el entrenamiento, ya que la regularización se realiza con la penalización a los pesos grandes. \n",
    "Los otros parámetros se pueden tomar: learning rate 0.05, momentum 0.3, 4000 evaluaciones del entrenamiento, cada una de 20 épocas, 6 neuronas en la capa intermedia.\n",
    "\n",
    "Entregue curvas de los tres errores (entrenamiento y test en una figura, penalización en otra figura) para el valor de gamma elegido, y para algún otro valor en que haya observado sobreajuste. Comente los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e11965-8298-4e97-b073-46594865beec",
   "metadata": {},
   "source": [
    "# Problema 5 - Dimensionalidad\n",
    "Repita el punto 4 del Práctico 1, usando ahora redes con 6 unidades en la capa intermedia. Los otros parámetros hay que setearlos adecuadamente, usando como guía los casos anteriores. Genere una gráfica que incluya los resultados de redes y árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5eba8-822f-4c8c-81a9-03dd34083dba",
   "metadata": {},
   "source": [
    "# Problema 6 - Multiclase\n",
    "Busque en los datasets de sklearn los archivos del problema iris y entrene una red sobre ellos. Tome un tercio de los datos como test, y los dos tercios restantes para entrenar y validar. Ajusto los parámetros de la red de manera conveniente. Realice curvas de entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d7cc2-deef-4104-a2db-f67407bcde8b",
   "metadata": {},
   "source": [
    "# Problema 7 - Minibatch\n",
    "La implementaciín de sklearn permite usar minibaths cambiando el parámetro batch_size. Realice una comparación de las curvas de aprendizaje para el problema de Sunspots, utilizando batches de longitud 1 y otros dos valores (3 curvas). Comente los resultados. Parametros del entrenamiento: 20% de validación, learning rate 0.05, momentum 0.3, 2000 evaluaciones del entrenamiento, cada una de 200 épocas, 6 neuronas en la capa intermedia. Los valores del minibatch se deben elegir para que en cada etapa de entrenamiento haya varias actualizaciones del gradiente, o sea que no debería ser mayor a 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46450af-4c89-48a9-b8b1-456d96885718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ia_venv)",
   "language": "python",
   "name": "ia_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
